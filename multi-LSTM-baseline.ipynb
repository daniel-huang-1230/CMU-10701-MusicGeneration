{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import sys\n",
    "import re \n",
    "import numpy as np \n",
    "import pandas as pd\n",
    "import music21\n",
    "from glob import glob\n",
    "import IPython\n",
    "from tqdm import tqdm\n",
    "import pickle\n",
    "from keras.utils import np_utils\n",
    "import os\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from music21 import converter, instrument, note, chord, stream"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "songs = glob('Jazz/*.mid')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def get_notes():\n",
    "    notes = []\n",
    "    for file in songs:\n",
    "        # converting .mid file to stream object\n",
    "        midi = converter.parse(file)\n",
    "        notes_to_parse = []\n",
    "        try:\n",
    "            # Given a single stream, partition into a part for each unique instrument\n",
    "            parts = instrument.partitionByInstrument(midi)\n",
    "        except:\n",
    "            pass\n",
    "        if parts: # if parts has instrument parts \n",
    "            notes_to_parse = parts.parts[0].recurse()\n",
    "            #print(notes_to_parse)\n",
    "        else:\n",
    "            notes_to_parse = midi.flat.notes\n",
    "            #print(notes_to_parse)\n",
    "        \n",
    "        #print('element')\n",
    "        for element in notes_to_parse: \n",
    "            #print(element)\n",
    "            if isinstance(element, note.Note):\n",
    "                # if element is a note, extract pitch\n",
    "                notes.append(str(element.pitch))\n",
    "            elif(isinstance(element, chord.Chord)):\n",
    "                # if element is a chord, append the normal form of the \n",
    "                # chord (a list of integers) to the list of notes. \n",
    "                notes.append('.'.join(str(n) for n in element.normalOrder))\n",
    "    with open('data/notes', 'wb') as filepath:\n",
    "        pickle.dump(notes, filepath)\n",
    "    #print('notes')\n",
    "    #print(notes)\n",
    "    return notes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def prepare_sequences(notes, n_vocab): \n",
    "    sequence_length = 100\n",
    "\n",
    "    # Extract the unique pitches in the list of notes.\n",
    "    pitchnames = sorted(set(item for item in notes))\n",
    "\n",
    "    # Create a dictionary to map pitches to integers\n",
    "    note_to_int = dict((note, number) for number, note in enumerate(pitchnames))\n",
    "\n",
    "    network_input = []\n",
    "    network_output = []\n",
    "    print(len(notes))\n",
    "    \n",
    "    # create input sequences and the corresponding outputs\n",
    "    for i in range(0, len(notes) - sequence_length, 1):\n",
    "        sequence_in = notes[i: i + sequence_length]\n",
    "        sequence_out = notes[i + sequence_length]\n",
    "        network_input.append([note_to_int[char] for char in sequence_in])\n",
    "        network_output.append(note_to_int[sequence_out])\n",
    "    \n",
    "    n_patterns = len(network_input)\n",
    "    \n",
    "    # reshape the input into a format comatible with LSTM layers \n",
    "    network_input = np.reshape(network_input, (n_patterns, sequence_length, 1))\n",
    "    \n",
    "    # normalize input\n",
    "    network_input = network_input / float(n_vocab)\n",
    "    \n",
    "    # one hot encode the output vectors\n",
    "    network_output = np_utils.to_categorical(network_output)\n",
    "    \n",
    "    return (network_input, network_output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from keras.models import Sequential\n",
    "from keras.layers import Activation, Dense, LSTM, Dropout, Flatten\n",
    "def create_network(network_in, n_vocab): \n",
    "    \"\"\"Create the model architecture\"\"\"\n",
    "    model = Sequential()\n",
    "    model.add(LSTM(128, input_shape=network_in.shape[1:], return_sequences=True))\n",
    "    model.add(Dropout(0.2))\n",
    "    model.add(LSTM(128, return_sequences=True))\n",
    "    model.add(Flatten())\n",
    "    model.add(Dense(256))\n",
    "    model.add(Dropout(0.3))\n",
    "    model.add(Dense(n_vocab))\n",
    "    model.add(Activation('softmax'))\n",
    "    model.compile(loss='categorical_crossentropy', optimizer='adam')\n",
    "\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from keras.callbacks import ModelCheckpoint\n",
    "def train(model, network_input, network_output, epochs): \n",
    "    \"\"\"\n",
    "    Train the neural network\n",
    "    \"\"\"\n",
    "    # Create checkpoint to save the best model weights.\n",
    "    filepath = 'weights.best.music3.hdf5'\n",
    "    checkpoint = ModelCheckpoint(filepath, monitor='loss', verbose=0, save_best_only=True)\n",
    "    \n",
    "    model.fit(network_input, network_output, epochs=epochs, batch_size=32, callbacks=[checkpoint])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def train_network():\n",
    "    \"\"\"\n",
    "    Get notes\n",
    "    Generates input and output sequences\n",
    "    Creates a model \n",
    "    Trains the model for the given epochs\n",
    "    \"\"\"\n",
    "    \n",
    "    epochs = 200\n",
    "    \n",
    "    notes = get_notes()\n",
    "    print('Notes processed')\n",
    "    \n",
    "    n_vocab = len(set(notes))\n",
    "    print('Vocab generated')\n",
    "    \n",
    "    network_in, network_out = prepare_sequences(notes, n_vocab)\n",
    "    print('Input and Output processed')\n",
    "    \n",
    "    model = create_network(network_in, n_vocab)\n",
    "    print('Model created')\n",
    "    return model\n",
    "    print('Training in progress')\n",
    "    train(model, network_in, network_out, epochs)\n",
    "    print('Training completed')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Notes processed\n",
      "Vocab generated\n",
      "732\n",
      "Input and Output processed\n",
      "Model created\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.engine.sequential.Sequential at 0x7f7fcc772850>"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_network()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'songs' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-4-a05916677f09>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mget_notes\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-3-a4ca617e9d7e>\u001b[0m in \u001b[0;36mget_notes\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mget_notes\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m     \u001b[0mnotes\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m     \u001b[0;32mfor\u001b[0m \u001b[0mfile\u001b[0m \u001b[0;32min\u001b[0m \u001b[0msongs\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      4\u001b[0m         \u001b[0;31m# converting .mid file to stream object\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m         \u001b[0mmidi\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mconverter\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mparse\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfile\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'songs' is not defined"
     ]
    }
   ],
   "source": [
    "get_notes()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
